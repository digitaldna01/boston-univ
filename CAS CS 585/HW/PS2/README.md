## CS585 Problem Set 2

### Instructions 

1.  Assignment is due at **11:59:59 PM on Tuesday March 04 2025**.

2.  Submission instructions:
    1. A single `.pdf` report that contains your work for Q1, Q2. For each individual question,
       we provide instruction for what should be included in your report. This should be submitted
       on gradescope. 
    2. For this assignment, we will use *gradescope autograder* for code submission and evaluation of your results on held-out test sets.
       For this to work, you will need to submit the code and results according to the following instructions:
       - Code and output files should be submitted to [Gradescope](https://www.gradescope.com)
         ```
         CS585_PS2.py
         Q1_label_predictions.npy
         Q2_surface_predictions.npy
         ```
       - Do not compress the files into `.zip` as this will not work.
       - Do not submit any `.ipynb` file from Colab, you can convert it to python files by `File → Download → Download .py`.
       - File and folder names are case sensitive.
       - *Not following this instruction to submit your code and results will lead to a loss of 100% of assignment points.*
       - You should primarily use the validation sets 
         for seeing how well you are doing and for picking hyper-parameters, and ideally we expect you to only submit the output
         of your final models to the test set.

### Google Colab and Dataset setup
In this assignment you will use [PyTorch](https://pytorch.org/), which is currently one of the most popular
deep learning frameworks and is very easy to pick up. It has a lot of tutorials and an active community answering
questions on its discussion forums. You will be using [Google Colab](https://colab.research.google.com/), a free environment
to run your experiments. Here are instructions on how to get started:

1. Upload the notebook to colab and then click on `Runtime → Change Runtime Type → Select GPU as
your hardware accelerator`.

2. In your Google Drive, create a new folder titled `CS_585_PS2`. This is the folder that will be mounted to Colab.
All outputs generated by Colab Notebook will be saved here.

3. Follow the instructions in the notebook to finish the setup.

4. Keep in mind that you need to keep your browser window open while running Colab. Colab does not allow
long-running jobs but it should be sufficient for the requirements of this assignment.

### Problems

1. **Implement and improve BaseNet on CIFAR-10 [30 pts].** For this part of the assignment, you will be working with
    the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. This dataset consists of 60K 32 × 32 color images from
    10 classes. There are 50K training images and 10K validation images. The images in CIFAR-10 are of size
    3 × 32 × 32, i.e. three-channel RGB images of 32 × 32 pixels.

    <div align="center"> <img src="cifar10.png" width="50%"> </div>

    a. **Implement BaseNet [5 pts].** Implement the BaseNet with the neural network shown below.
       The starter code for this is in the BaseNet class. After implementing
       the BaseNet class, you can run the code with default settings to get a
       baseline accuracy of around 60% on the validation set. The BaseNet is
       built with following components:
       * Convolutional, i.e. `nn.Conv2d`
       * Pooling, e.g. `nn.MaxPool2d`
       * Fully-connected (linear), i.e. `nn.Linear`
       * Non-linear activations, e.g. `nn.ReLU`
       BaseNet consists of two convolutional modules (conv-relu-maxpool) and
       two linear layers. The precise architecture is defined below:

       | Layer No. | Layer Type  | Kernel Size | Input Dim | Output Dim  | Input Channels | Output Channels |
       | --------- | ----------- | ----------- | --------- | ----------- | -------------- | --------------- |
       | 1         | conv2d      | 5           | 32        | 28          | 3              | 6               |
       | 2         | relu        | -           | 28        | 28          | 6              | 6               |
       | 3         | maxpool2d   | 2           | 28        | 14          | 6              | 6               |
       | 4         | conv2d      | 5           | 14        | 10          | 6              | 16              |
       | 5         | relu        | -           | 10        | 10          | 16             | 16              |
       | 6         | maxpool2d   | 2           | 10        | 5           | 16             | 16              |
       | 7         | linear      | -           | 1         | 1           | 400            | 200             |
       | 8         | relu        | -           | 1         | 1           | 200            | 200             |
       | 9         | linear      | -           | 1         | 1           | 200            | 10              |

       **In your report, include:** your model by using Python print command
       `print(net)` and final accuracy on the validation set.

    b. **Improve BaseNet [20 pts].** 
       Your goal is to edit the BaseNet class or make new classes for devising
       a more accurate deep net architecture.  In your report, you will need to
       include a table similar to the one above to illustrate your final
       network.
       
       Before you design your own architecture, you should start by getting
       familiar with the BaseNet architecture already provided, the meaning of
       hyper-parameters and the function of each layer. This
       [tutorial](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)
       by PyTorch is helpful for gearing up on using deep nets. Also, see
       Andrej Karpathy's lectures on
       [CNNs](http://cs231n.github.io/convolutional-networks/) and [neural
       network training](http://cs231n.github.io/neural-networks-3/).

       For improving the network, you should consider the following aspects.
       In addition, you can also try out your own ideas. Since Colab makes only
       limited computational resources available, we encourage you to
       rationally limit training time and model size. *Do not simply just copy
       over model architectures from the Internet.*

       * **Data normalization.** Normalizing input data makes training easier
       and more robust. You can normalize the data to made it zero mean and fixed standard
       deviation ($`\sigma = 1`$ is the go-to choice).  You may use
       `transforms.Normalize()` with the right parameters for this data
       normalization. After your edits, make sure that `test_transform` has the
       same data normalization parameters as `train_transform`.
       * **Data augmentation.** Augment the training data using random crops,
       horizontal flips, etc. You may find functions `transforms.RandomCrop()`,
       `transforms.RandomHorizontalFlip()` useful. Remember, you shouldn't
       augment data at test time. You may find the [PyTorch
       tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#transforms)
       on transforms useful.
       * **Deeper network.** Experiment by adding more convolutional and fully
       connected layers. Add more convolutional layers with increasing output
       channels and also add more linear (fully convolutional) layers.
       * **Normalization layers.** [Normalization
       layers](https://pytorch.org/docs/master/nn.html#normalization-functions)
       may help reduce overfitting and improve training of the model. Add
       normalization layers after conv layers (`nn.BatchNorm2d`). Add
       normalization layers after linear layers and experiment with inserting
       them before or after ReLU layers (`nn.BatchNorm1d`).

       **In your report, include:** 
       - Your best model. Include final accuracy on validation set, table
         defining your final architecture (similar to the BaseNet table above),
         training loss plot and test accuracy plot for final model
         (auto-generated by the notebook). A reasonable submission with more
         than 85% accuracy will be given full credit for this part.
       - An ablation table, listing all factors that you tried to make
         improvement to your final model as well as the corresponding validation 
         accuracy.

    c. **Secret test set [5 pts].** We also have a secret test set containing
       2000 images similar to CIFAR-10. We provide code that saves your model predictions to a
       `predictions.npy` file. Submit the prediction for your best model to
       gradescope.

       **In your report, include:** Test set accuracy (category-wise and aggregate) 
       for your best model. You can get this from gradescope. A reasonable submission 
       with more than 70% accuracy will be given full credit for this part.

2. **Surface normal [40 pts].** In this part, you will build your own surface
    normal estimation model on a subset of the [Taskonomy
    dataset](http://taskonomy.stanford.edu/). This task comprises of predicting the
    normal vector at every pixel location. We will be using the mean and median
    angular error as well as accuracy at 11.25$`^{o}`$, 22.5$`^{o}`$, 30$`^{o}`$ to
    measure performance. We provide code for computing these metrics.

    <div align="center">
    <img src="point_1_view_6_domain_rgb.png" width="30%">
    <img src="point_1_view_6_domain_normal.png" width="30%">
    </div>

    **Data.** We have 400 images for training, and 200 images for testing. Each image is
    128x128. We provide a basic data loader that you can build upon.

    **What you need to do:**
    
    a. **Implement training cycle:** Unlike Part 1 where we provided you with 
       the training cycle, you will implement it all by yourself this time.
       Make sure you evaluate metrics and loss on the validation set every so
       often to check for overfitting.
    b. **Build on top of ImageNet pre-trained Model [15 pts]:** Once you have a
       training cycle set up, you should design models for solving the task. To
       make training faster, we will build a model on top of a ResNet-18 [^1]
       model that has been pre-trained on the ImageNet dataset (via
       `models.resnet18(pretrained=True)`).  These models are trained to predict
       the 1000 ImageNet object classes. To use this model for surface normal
       estimation, you will have to remove the classifier and global average
       pooling layers, and stack on additional layers for surface normal
       estimation. Note that, ResNet-18 model downsamples the input image by a
       factor of 32, remeber to upsample your prediction using bilinear
       interpolation.  Since surface normal estimation is not a classification
       task anymore, you should also play with the loss function.  Example of
       loss functions that you can try are: L1 loss, cosine similarity loss.
       You can refer to [^2] [^3] for inspiration for how
       you can build on top of such pre-existing models. Again, carefully
       document the design choices you make in your report.  
       
       For reference, we provide the results obtained from a very simple model.

       Below is the performance of ResNet-18 model.

       | mean angular error (lower the better)   | median angular error (lower the better)  | accuracies at 11.25 degree (higher the better) | accuracies at 22.5 degree (higher the better)   | accuracies at 30 degree (higher the better)  | 
       | ----------- | ----------- | ----------- | ----------- | ----------- | 
       | 38.6 | 34.7 | 10.6% | 30.1% | 42.6% | 

       Here are some sample prediction outputs using ResNet-18 model.
       
       <div align="center"> <img src="visualization_r18.png" width="100%"> </div>

       **In your report, include:** your model in the report by using Python
       print command `print(net)`, and final performance on validation set (all 5
       metrics).

    c. **Increase your model output resolution [15 pts]:** The current model
       simply replaces the final fully connected layer in a ResNet-18 model for
       surface normal estimation. This still has a lot of draw backs. The
       most important factors that causes poor performance is the low output
       resolution. ResNet-18 model for image classification downsamples the input
       image by a factor of 32. In the previous step, we recover the resolution by
       a simple bilinear interpolation layer which upsamples the prediction by 32
       times. In this part, our goal is to explore other choices to generate
       high-resolution predictions. We offer two choices that you can consider:
       * **Atrous (or dilated) convolution.** The concept of atrous convolution 
         (or dilated convolution) is described it the DeepLab paper[^4].
         One way to think about atrous convolution is to think of running the 
         network on shifted versions of the image and weaving the outputs together
         to produce an output at the original spatial resolution. This can be done
         simply by using the `dilataion` arguments in PyTorch. Refer to the paper[^4] 
         and PyTorch documentation for more detail.
       * **Building a learned upsampler.**  Instead of using bilinear upsampling,
         you can use a decoder that *learns* to upsample 
         prediction to improve the output quality. The key that
         makes decoder works better than bilinear interpolation is the usage of
         *skip-connection*.  Refer to the U-net[^3] and DeepLabv3+[^5] for
         more detail.

        You can implement either of these two choices, or other choices you
        find in other papers to increase your model output resolution. Please
        describe the methods you try in your report and report their
        performance.

       **In your report, include:**
       - Your best model. Include final performance on validation set (5 metrics).
       - An ablation table, listing all factors that you tried to make
         improvement to your final model as well as the validation performance.


    d. **Visualize your prediction [5 pts]:** In your report, visualize
       5 predictions of your model on images that are not in the dataset. We
       provide functions to visualize arbitrary images. You just need to place
       your images under `./data/normal_visualization/images` and run the appropriate
       code cell in colab. Take images
       of your choice and test the model with them. We also provide
       some example images under [normal_visualization](normal_visualization).

       **In your report, include:** 
       - visualization of model prediction on five of your favorite indoor images
       - visual comparisons of the output from part 2 and 3 on 2 images from the 
         validation dataset, discuss your observations.

    e. **Secret test set [5 pts].** 
       We have created a secret test set containing 200 images of indoor pictures
       from the collierville subset. We provide code that saves your model predictions to a
       `Q2_normal_predictions` file. Submit the prediction for your best
       model to gradescope.
       
       **In your report, include:** test set accuracy of your best model. 
       You can get this from gradescope.

### References
[^1]: Kaiming He et al. "Deep residual learning for image recognition." CVPR 2016.
[^2]: Long, Jonathan, Evan Shelhamer, and Trevor Darrell. "Fully convolutional networks for semantic segmentation." CVPR 2015.
[^3]: Olaf Ronneberger et al. "U-net: Convolutional networks for biomedical image segmentation." MICCAI 2015.
[^4]: Liang-Chieh Chen et al. "Semantic image segmentation with deep convolutional nets and fully connected CRFs." ICLR 2015.
[^5]: Liang-Chieh Chen et al. "Encoder-decoder with atrous separable convolution for semantic image segmentation." ECCV 2018.
