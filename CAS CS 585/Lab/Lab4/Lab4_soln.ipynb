{"cells":[{"cell_type":"markdown","id":"22efc602","metadata":{"id":"22efc602"},"source":["# Lab 4"]},{"cell_type":"markdown","id":"973b3678","metadata":{"id":"973b3678"},"source":["In this lab section, you will practice implementing the forward and back-propagation of a simple neural network in numpy. A neural network can be thought of as a composition of functions where each layer represents a function. During the forward pass, the input is passed through each layer sequentially. During the backward pass, we compute the derivations with respect to the parameters in each layer through chain derivation. You will be implementing classes for a fully-connected layer and the sigmoid activation function as well as the mean squared error."]},{"cell_type":"markdown","id":"ca325ba0","metadata":{"id":"ca325ba0"},"source":["# Implement the backwards function of a linear layer class"]},{"cell_type":"markdown","id":"6b19f7c3","metadata":{"id":"6b19f7c3"},"source":["Compute the gradient of the linear transformation layer in the backward function."]},{"cell_type":"code","execution_count":null,"id":"42f505bc","metadata":{"id":"42f505bc"},"outputs":[],"source":["class Linear():\n","    '''Linear layer. Parameter is NxM matrix L, input is matrix v of size B x N\n","    where B is batch size, output is vL.'''\n","\n","    def __init__(self, input_dim, output_dim, name=\"Linear\", std=1e-1):\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.weights = std * np.random.normal(size=(input_dim, output_dim))\n","        self.grad = None\n","\n","    def forward(self, input): \n","        self.input = input  \n","        return np.dot(input, self.weights)\n","\n","    def backward(self, downstream_grad):\n","        '''downstream_grad should be NxB.'''\n","        if len(downstream_grad.shape) != 2:\n","            downstream_grad = np.reshape(\n","                downstream_grad, (len(downstream_grad), 1))\n","        self.grad = np.dot(self.input.T, downstream_grad)\n","        return np.dot(downstream_grad, self.weights.transpose())"]},{"cell_type":"markdown","id":"83866f4e","metadata":{"id":"83866f4e"},"source":["# Implement the backwards function of a sigmoid layer class"]},{"cell_type":"code","execution_count":null,"id":"e4ac5c27","metadata":{"id":"e4ac5c27"},"outputs":[],"source":["class Sigmoid():\n","    '''Sigmoid layer.'''\n","\n","    def __init__(self, name=\"Sigmoid\"):\n","        self.name = name\n","        self.grad = None\n","\n","    def forward(self, input):\n","        self.output = np.exp(input) / (1.0 + np.exp(input))\n","        return self.output\n","\n","    def backward(self, downstream_grad):\n","        ## -- ! code required  \n","        return (self.output - self.output**2) * downstream_grad "]},{"cell_type":"markdown","id":"b1033ac2","metadata":{"id":"b1033ac2"},"source":["# Implement the backwards function of a mean squared errror class"]},{"cell_type":"markdown","id":"901fd4aa","metadata":{"id":"901fd4aa"},"source":["We define the mean square error as follows: \n","<p>\n","$MSE(\\hat y) = \\frac{1}{2N}\\sum_{i=1}^N(y_i - \\hat y_i)^2$. \n","</p> \n","where $y$ is the label and $\\hat y$ is your prediction. Compute the gradient of MSE w.r.t $\\hat y$ in the backwards function."]},{"cell_type":"code","execution_count":null,"id":"6481e975","metadata":{"id":"6481e975"},"outputs":[],"source":["class MeanSquaredError():\n","    '''cross entropy loss.'''\n","    def __init__(self, labels, name=\"Mean Squared Error\"):\n","        self.name = name\n","        self.labels = labels\n","        \n","    def forward(self, input):\n","        '''input is BxN, output is B'''\n","        ## -- ! code required  \n","        self.input = input\n","        return np.dot((self.labels - self.input).T, (self.labels - self.input))/(2*self.input.shape[0])\n","\n","    def backward(self, downstream_grad):\n","        ## -- ! code required  \n","        grad = -(self.labels -self.input)/self.input.shape[0]\n","        return grad * downstream_grad"]},{"cell_type":"markdown","id":"df3f6324","metadata":{"id":"df3f6324"},"source":["## Implementing a simple MLP.\n","\n","In this section, we will develop a shallow neural network with fully-connected layers, aka Multi-Layer Perceptron (MLP) using the layers that have already been defined. Below, we initialize toy data that we will use to develop your implementation."]},{"cell_type":"code","execution_count":null,"id":"957eb2c6","metadata":{"id":"957eb2c6","outputId":"87bc9153-9d33-4ff5-9e06-4991e0678c2e"},"outputs":[{"name":"stdout","output_type":"stream","text":["X =  (100, 1)\n","y =  (100, 1)\n"]}],"source":["# setup\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Create some toy data\n","X = np.linspace(-1, 1, 100).reshape(-1,1)\n","y = 5*X + 2 + 0.5*np.random.normal()\n","\n","print ('X = ', X.shape)\n","print('y = ', y.shape)"]},{"cell_type":"markdown","id":"6e7b7429","metadata":{"id":"6e7b7429"},"source":["Complete the loss function where you can call "]},{"cell_type":"code","execution_count":null,"id":"acc222ef","metadata":{"id":"acc222ef"},"outputs":[],"source":["def forward_layers(layers, input):\n","    '''Forward pass on all the layers. Must be called before backwards pass.'''\n","    output = input\n","    for layer in layers:\n","        output = layer.forward(output)\n","    #assert output.size == 1, \"only supports computations that output a scalar!\"\n","    return output\n","\n","class TwoLayerMLP(object):\n","    def __init__(self, input_size, hidden_size, label_size, std=1e-1, activation='sigmoid'):\n","        np.random.seed(0)\n","        self.input_size = input_size\n","        self.num_classes = num_classes\n","        \n","        self.params = {}\n","        self.models = [\n","                  Linear(input_size, hidden_size),\n","                  Sigmoid(),\n","                  Linear(hidden_size, label_size),\n","                ]\n","        \n","        self.activation = 'sigmoid' \n","        self.params['W1'] = self.models[0].weights\n","        self.params['W2'] = self.models[-1].weights\n","           \n","    def loss(self, X, y=None, reg=0.0):\n","        # Unpack variables from the params dictionary\n","        W1 = self.params['W1']\n","        W2 = self.params['W2']\n","        _, C = W2.shape\n","        N, D = X.shape\n","\n","        ###########################################################################\n","        # Computes the loss\n","        ###########################################################################  \n","      \n","        scores = forward_layers(self.models, X)  \n","        loss_layer  = MeanSquaredError(y)   \n","        loss = loss_layer.forward(scores)        \n","\n","        grads = {}\n","        ###########################################################################\n","        # TODO: Compute the backward pass, computing the derivatives of the weights\n","        # and biases. Store the results in the grads dictionary. For example,\n","        # grads['W1'] should store the gradient on W1, and be a matrix of same size\n","        ###########################################################################\n","        \n","        self.backward_layers(loss_layer.backward(np.array([1]))) \n","        grads['W2'] = self.models[-1].grad\n","        grads['W1'] = self.models[0].grad\n","        ###########################################################################\n","        #                            END OF YOUR CODE\n","        ###########################################################################\n","        return loss, grads\n","\n","    def backward_layers(self, downstream_grad):\n","        '''runs a backward pass on all the layers.\n","        after this function is finished, look at layer.grad to find the\n","        gradient with respect to that layer's parameter.'''\n","        for layer in reversed(self.models):\n","            downstream_grad = layer.backward(downstream_grad)\n","\n","    def train(self, X, y, X_val, y_val,\n","            learning_rate=1e-3, learning_rate_decay=0.95,\n","            reg=1e-5, num_epochs=10,\n","            batch_size=1, verbose=False):\n","\n","        num_train = X.shape[0]\n","        iterations_per_epoch = 1 #int(max(num_train / batch_size, 1))\n","        epoch_num = 0\n","\n","        # Use SGD to optimize the parameters in self.model\n","        loss_history = []\n","        grad_magnitude_history = []\n","        train_acc_history = []\n","        val_acc_history = []\n","\n","        np.random.seed(1)\n","        for epoch in range(num_epochs):\n","            # fixed permutation (within this epoch) of training data\n","            perm = np.random.permutation(num_train)\n","\n","            # go through minibatches\n","            for it in range(iterations_per_epoch):\n","                X_batch = None\n","                y_batch = None\n","\n","                # Create a random minibatch\n","                idx = perm[it*batch_size:(it+1)*batch_size]\n","                X_batch = X[idx, :]\n","                y_batch = y[idx]\n","                # Compute loss and gradients using the current minibatch\n","                loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n","                #print(\"loss\", loss)\n","                loss_history.append(loss)\n","\n","                # do gradient descent\n","                for param in self.params:\n","                    self.params[param] -= grads[param] * learning_rate\n","\n","                # record gradient magnitude (Frobenius) for W1\n","                grad_magnitude_history.append(np.linalg.norm(grads['W1']))\n","\n","            # Decay learning rate\n","            learning_rate *= learning_rate_decay\n","\n","        return {\n","          'loss_history': loss_history,\n","          'grad_magnitude_history': grad_magnitude_history, \n","        }"]},{"cell_type":"code","execution_count":null,"id":"6549172f","metadata":{"id":"6549172f","outputId":"b1000a82-a7c1-4448-bb6a-a41e6d70c2ae"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1) Your scores:\n","\n","2.0038506582894944\n","\n","\n","Difference between your scores and correct scores:\n","6.582894944706652e-07\n","\n","\n","(2) Your loss: 5.941686\n","Difference between your loss and correct loss:\n","0.9416859499876624\n"]}],"source":["input_size = 1\n","hidden_size = 10\n","num_classes = 1\n","\n","net = TwoLayerMLP(input_size, hidden_size, num_classes)\n","scores = forward_layers(net.models, X)\n","print ('(1) Your scores:\\n')\n","print (np.linalg.norm(scores))\n","print ('\\n')\n","correct_norm = 2.00385\n","# # The difference should be very small (< 1e-4)\n","print ('Difference between your scores and correct scores:')\n","print (np.sum(np.abs(np.linalg.norm(scores) -correct_norm)))\n","print ('\\n')\n","\n","loss, _ = net.loss(X, y, reg=0.1)\n","correct_loss = 5\n","\n","# Since we generate random data your loss would not be the same as the correct loss.\n","# However, the difference should fairly small (less than 1 or 2)\n","print ('(2) Your loss: %f'%(loss))\n","print ('Difference between your loss and correct loss:')\n","print (np.sum(np.abs(loss - correct_loss)))"]},{"cell_type":"code","execution_count":null,"id":"4fe7dd80","metadata":{"id":"4fe7dd80","outputId":"0f742522-317d-484a-9ce3-b89e2b4b7db0"},"outputs":[{"name":"stdout","output_type":"stream","text":["W2 max relative error: 7.498745e-11\n","W1 max relative error: 3.029077e-09\n"]}],"source":["# Use numeric gradient checking to check your implementation of the backward pass.\n","# If your implementation is correct, the difference between the numeric and\n","# analytic gradients should be less than 1e-8 for each of W1, W2, b1, and b2.\n","def rel_error(x, y):\n","    \"\"\" returns relative error \"\"\"\n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n","\n","\n","def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n","  \"\"\" \n","  a naive implementation of numerical gradient of f at x \n","  - f should be a function that takes a single argument\n","  - x is the point (numpy array) to evaluate the gradient at\n","  \"\"\" \n","\n","  fx = f(x) # evaluate function value at original point\n","  grad = np.zeros_like(x)\n","  # iterate over all indexes in x\n","  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n","  while not it.finished:\n","\n","    # evaluate function at x+h\n","    ix = it.multi_index\n","    oldval = x[ix]\n","    x[ix] = oldval + h # increment by h\n","    fxph = f(x) # evalute f(x + h)\n","    x[ix] = oldval - h\n","    fxmh = f(x) # evaluate f(x - h)\n","    x[ix] = oldval # restore\n","\n","    # compute the partial derivative with centered formula\n","    grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n","    if verbose:\n","      print (ix, grad[ix])\n","    it.iternext() # step to next dimension\n","\n","  return grad\n","\n","loss, grads = net.loss(X, y, reg=0.1)\n","\n","# these should all be very small\n","for param_name in grads:\n","    f = lambda W: net.loss(X, y, reg=0.1)[0]\n","    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n","    print ('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"]},{"cell_type":"code","execution_count":null,"id":"edd348f2","metadata":{"id":"edd348f2"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}